{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'hdfs://orion11:14001/reddit/2012/RC_2012-01.bz2'\n",
    "# path = 'hdfs://orion11:14001/reddit/2017/RC_2017-01.bz2'\n",
    "path = 'hdfs://orion11:14001/reddit/sampled_reddit'\n",
    "# path_all = 'hdfs://orion11:14001/reddit/*/*.bz2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(archived=True, author='halftone84', author_cakeday=None, author_flair_css_class=None, author_flair_text=None, body='I read the title and thought of that cheating bitch clown from the glassjaw video', body_html=None, controversiality=0, created=None, created_utc='1309478410', distinguished=None, downs=0, edited='false', gilded=0, id='c22x4bc', link_id='t3_idgji', mod_reports=None, name='t1_c22x4bc', parent_id='t3_idgji', removal_reason=None, replies=None, retrieved_on=1427302517, saved=None, score=1, score_hidden=False, stickied=None, subreddit='AskReddit', subreddit_id='t5_2qh1i', ups=1, user_reports=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.json(path)\n",
    "# df = spark.read.json(path_all)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('reddit_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = '''\n",
    "# SELECT \n",
    "#     subreddit_id\n",
    "# FROM\n",
    "#    reddit_data\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(query).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm up Q1\n",
    "\n",
    "# query = '''\n",
    "# SELECT \n",
    "#     COUNT(DISTINCT subreddit_id) count\n",
    "# FROM\n",
    "#    reddit_data\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(query).show(3) \n",
    "#  78201 (reddit_201701)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(archived=True, author='halftone84', author_cakeday=None, author_flair_css_class=None, author_flair_text=None, body='I read the title and thought of that cheating bitch clown from the glassjaw video', body_html=None, controversiality=0, created=None, created_utc='1309478410', distinguished=None, downs=0, edited='false', gilded=0, id='c22x4bc', link_id='t3_idgji', mod_reports=None, name='t1_c22x4bc', parent_id='t3_idgji', removal_reason=None, replies=None, retrieved_on=1427302517, saved=None, score=1, score_hidden=False, stickied=None, subreddit='AskReddit', subreddit_id='t5_2qh1i', ups=1, user_reports=None),\n",
       " Row(archived=True, author='keiyakins', author_cakeday=None, author_flair_css_class=None, author_flair_text=None, body=\"Because you're about to wash your hands, but not your feet.\", body_html=None, controversiality=0, created=None, created_utc='1309478412', distinguished=None, downs=0, edited='false', gilded=0, id='c22x4bi', link_id='t3_idez1', mod_reports=None, name='t1_c22x4bi', parent_id='t3_idez1', removal_reason=None, replies=None, retrieved_on=1427302517, saved=None, score=1, score_hidden=False, stickied=None, subreddit='AskReddit', subreddit_id='t5_2qh1i', ups=1, user_reports=None),\n",
       " Row(archived=True, author='chuck_finley17', author_cakeday=None, author_flair_css_class=None, author_flair_text=None, body=\"All he had was a college id. You're supposed to have a government issued photo id where the name matches exactly the name on the boarding pass. I'd say the blame should fall squarely on the TSA. If I can't even go to the gate anymore to meet people coming into town how the hell do they explain him getting all they way onto a plane before anyone notices?\", body_html=None, controversiality=0, created=None, created_utc='1309478416', distinguished=None, downs=0, edited='false', gilded=0, id='c22x4bw', link_id='t3_idilf', mod_reports=None, name='t1_c22x4bw', parent_id='t1_c22worq', removal_reason=None, replies=None, retrieved_on=1427302517, saved=None, score=3, score_hidden=False, stickied=None, subreddit='politics', subreddit_id='t5_2cneq', ups=3, user_reports=None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archived',\n",
       " 'author',\n",
       " 'author_cakeday',\n",
       " 'author_flair_css_class',\n",
       " 'author_flair_text',\n",
       " 'body',\n",
       " 'body_html',\n",
       " 'controversiality',\n",
       " 'created',\n",
       " 'created_utc',\n",
       " 'distinguished',\n",
       " 'downs',\n",
       " 'edited',\n",
       " 'gilded',\n",
       " 'id',\n",
       " 'link_id',\n",
       " 'mod_reports',\n",
       " 'name',\n",
       " 'parent_id',\n",
       " 'removal_reason',\n",
       " 'replies',\n",
       " 'retrieved_on',\n",
       " 'saved',\n",
       " 'score',\n",
       " 'score_hidden',\n",
       " 'stickied',\n",
       " 'subreddit',\n",
       " 'subreddit_id',\n",
       " 'ups',\n",
       " 'user_reports']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- archived: boolean (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_cakeday: boolean (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- body_html: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created: long (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- downs: long (nullable = true)\n",
      " |-- edited: string (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- mod_reports: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- removal_reason: string (nullable = true)\n",
      " |-- replies: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- saved: boolean (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- score_hidden: boolean (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      " |-- user_reports: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+\n",
      "|        subreddit|                body|\n",
      "+-----------------+--------------------+\n",
      "|        AskReddit|I read the title ...|\n",
      "|        AskReddit|Because you're ab...|\n",
      "|         politics|All he had was a ...|\n",
      "|        AskReddit|Flushing with you...|\n",
      "|             IAmA|I grew up in Texa...|\n",
      "|       California|I did this alread...|\n",
      "|             pics|Ctrl-F: *drr* *dr...|\n",
      "|       reddit.com|Damn. I was wrong...|\n",
      "|        AskReddit|          Me too. :/|\n",
      "|        AskReddit|I don't know, but...|\n",
      "|           trance|How did you do that?|\n",
      "|           google|I can haz google+...|\n",
      "|             IAmA|Months?  That's a...|\n",
      "|         sandiego|Is that the Reddi...|\n",
      "|       googleplus|I got my invite, ...|\n",
      "|       technology|This sounds bette...|\n",
      "|        worldnews|Israel handled it...|\n",
      "|DrawingFromWithin|Yeah, I'm ruler g...|\n",
      "|           gaming|I just found out ...|\n",
      "|       reddit.com|Hope you can't wa...|\n",
      "+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT\n",
    "  subreddit, \n",
    "  body\n",
    "FROM\n",
    "  reddit_data\n",
    "WHERE\n",
    "  body != \"[deleted]\"\n",
    "'''\n",
    "text_all = spark.sql(query)\n",
    "text_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    return (row['subreddit'], row['body'].split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 163 in stage 5.0 failed 1 times, most recent failure: Lost task 163.0 in stage 5.0 (TID 4407, localhost, executor driver): java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:51)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)\n\tat sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:612)\n\tat org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)\n\tat org.apache.spark.util.Utils.copyFileStreamNIO(Utils.scala)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.mergeSpillsWithTransferTo(UnsafeShuffleWriter.java:457)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.mergeSpills(UnsafeShuffleWriter.java:318)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.closeAndWriteOutput(UnsafeShuffleWriter.java:237)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:190)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:51)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)\n\tat sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:612)\n\tat org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)\n\tat org.apache.spark.util.Utils.copyFileStreamNIO(Utils.scala)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.mergeSpillsWithTransferTo(UnsafeShuffleWriter.java:457)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.mergeSpills(UnsafeShuffleWriter.java:318)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.closeAndWriteOutput(UnsafeShuffleWriter.java:237)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:190)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6197440bd596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 163 in stage 5.0 failed 1 times, most recent failure: Lost task 163.0 in stage 5.0 (TID 4407, localhost, executor driver): java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:51)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)\n\tat sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:612)\n\tat org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)\n\tat org.apache.spark.util.Utils.copyFileStreamNIO(Utils.scala)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.mergeSpillsWithTransferTo(UnsafeShuffleWriter.java:457)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.mergeSpills(UnsafeShuffleWriter.java:318)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.closeAndWriteOutput(UnsafeShuffleWriter.java:237)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:190)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: No space left on device\n\tat sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n\tat sun.nio.ch.FileDispatcherImpl.write(FileDispatcherImpl.java:60)\n\tat sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)\n\tat sun.nio.ch.IOUtil.write(IOUtil.java:51)\n\tat sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:211)\n\tat sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:516)\n\tat sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:612)\n\tat org.apache.spark.util.Utils$.copyFileStreamNIO(Utils.scala:389)\n\tat org.apache.spark.util.Utils.copyFileStreamNIO(Utils.scala)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.mergeSpillsWithTransferTo(UnsafeShuffleWriter.java:457)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.mergeSpills(UnsafeShuffleWriter.java:318)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.closeAndWriteOutput(UnsafeShuffleWriter.java:237)\n\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:190)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "words = text_all.rdd.map(f).reduceByKey(lambda a, b: a + b)\n",
    "words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                body|\n",
      "+--------------------+\n",
      "|I read the title ...|\n",
      "|Because you're ab...|\n",
      "|All he had was a ...|\n",
      "|Flushing with you...|\n",
      "|I grew up in Texa...|\n",
      "|I did this alread...|\n",
      "|Ctrl-F: *drr* *dr...|\n",
      "|Damn. I was wrong...|\n",
      "|          Me too. :/|\n",
      "|I don't know, but...|\n",
      "|           [deleted]|\n",
      "|How did you do that?|\n",
      "|I can haz google+...|\n",
      "|Months?  That's a...|\n",
      "|Is that the Reddi...|\n",
      "|I got my invite, ...|\n",
      "|This sounds bette...|\n",
      "|Israel handled it...|\n",
      "|Yeah, I'm ruler g...|\n",
      "|I just found out ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT \n",
    "    body\n",
    "FROM\n",
    "   reddit_data\n",
    "'''\n",
    "test = spark.sql(query)\n",
    "test.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# query = '''\n",
    "# SELECT\n",
    "#   subreddit, \n",
    "#   body\n",
    "# FROM\n",
    "#    reddit_data\n",
    "# WHERE \n",
    "#   subreddit = 'reddit.com'\n",
    "# '''\n",
    "# test = spark.sql(query)\n",
    "# test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|      subreddit|\n",
      "+---------------+\n",
      "|          anime|\n",
      "|         travel|\n",
      "|     MensRights|\n",
      "|   SaltLakeCity|\n",
      "|           udel|\n",
      "|       lacrosse|\n",
      "|         AdPorn|\n",
      "|        Amateur|\n",
      "|     ArtHistory|\n",
      "|   bachelorchef|\n",
      "|           Utah|\n",
      "|        jewelry|\n",
      "|     QuotesPorn|\n",
      "|  sunshinecoast|\n",
      "|youtubecomments|\n",
      "|    creepypasta|\n",
      "|          HPMOR|\n",
      "|          jrotc|\n",
      "|     TheKilling|\n",
      "|     WahoosTipi|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT\n",
    "  DISTINCT subreddit\n",
    "FROM\n",
    "  reddit_data\n",
    "WHERE\n",
    "  body != \"[deleted]\"\n",
    "'''\n",
    "text_subreddits = spark.sql(query)\n",
    "text_subreddits.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[subreddit: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anime',\n",
       " 'travel',\n",
       " 'MensRights',\n",
       " 'SaltLakeCity',\n",
       " 'udel',\n",
       " 'lacrosse',\n",
       " 'AdPorn',\n",
       " 'Amateur',\n",
       " 'ArtHistory',\n",
       " 'bachelorchef',\n",
       " 'Utah',\n",
       " 'jewelry',\n",
       " 'QuotesPorn',\n",
       " 'sunshinecoast',\n",
       " 'youtubecomments',\n",
       " 'creepypasta',\n",
       " 'HPMOR',\n",
       " 'jrotc',\n",
       " 'TheKilling',\n",
       " 'WahoosTipi',\n",
       " 'freelanceWriters',\n",
       " 'RecruitBigDanV',\n",
       " 'Whippets',\n",
       " 'couchsurfing',\n",
       " 'costa_rica',\n",
       " 'bookshelf',\n",
       " 'BCCHigh',\n",
       " 'electroswing',\n",
       " 'charteroak',\n",
       " 'NIU',\n",
       " 'marvelheroes',\n",
       " 'ASRoma',\n",
       " 'ukraina',\n",
       " 'TrueOffMyChest',\n",
       " 'kitchener',\n",
       " 'redditisfun',\n",
       " 'mistyfront',\n",
       " 'harasser',\n",
       " 'MaddenMobile',\n",
       " 'electrical',\n",
       " 'UnresolvedMysteries',\n",
       " 'Goldfish',\n",
       " 'StLouisBeer',\n",
       " 'DIYBeauty',\n",
       " 'DRKCoin',\n",
       " 'left4dead',\n",
       " 'realitytv',\n",
       " 'OnlyFoolsAndHorses',\n",
       " 'Entrepreneurship',\n",
       " 'Warcraft',\n",
       " 'c0daisajerk',\n",
       " 'ItsNotOnion',\n",
       " 'UVU',\n",
       " 'UAB',\n",
       " 'washyourcigs',\n",
       " 'HoJang',\n",
       " 'Worcester',\n",
       " 'Nightmares',\n",
       " 'debunked',\n",
       " 'lojban',\n",
       " 'redditresort',\n",
       " 'de4n',\n",
       " 'htconex',\n",
       " 'SCW',\n",
       " 'metro2033',\n",
       " 'MexicoIAMA',\n",
       " 'DotCom',\n",
       " 'Fairbanks',\n",
       " 'ProjectRS06',\n",
       " 'londonfootballmeetup',\n",
       " 'contextfreepete',\n",
       " 'Daleks',\n",
       " 'Jobopenings',\n",
       " 'rosie_jones',\n",
       " 'AchievementsBoosting',\n",
       " 'strawberry',\n",
       " 'pokewalker',\n",
       " 'IUPUI',\n",
       " 'AntiFap',\n",
       " 'TheodoreRoosevelt',\n",
       " 'Surged',\n",
       " 'Banksy',\n",
       " 'GoneWildPlusSize',\n",
       " 'BadMusicVideos',\n",
       " 'battlefront',\n",
       " 'e3expo',\n",
       " 'NOTSONEWREDDITS',\n",
       " 'WedgieFetish',\n",
       " 'ComputerNetworks',\n",
       " 'secularbuddhism',\n",
       " 'tf2dm',\n",
       " 'PostMarxism',\n",
       " 'linuxmemes',\n",
       " 'MLBTheShow',\n",
       " 'Solr',\n",
       " 'pools',\n",
       " 'CNU',\n",
       " 'WizardWars',\n",
       " 'DunedinFlorida',\n",
       " 'utahfootball',\n",
       " 'itt',\n",
       " 'rwmc',\n",
       " 'UniConcepts',\n",
       " 'Megaupload',\n",
       " 'Haberdashers',\n",
       " 'laaabaseball',\n",
       " 'anomynous',\n",
       " 'A1B21F8244F',\n",
       " 'runkeeper',\n",
       " 'SelfTrees',\n",
       " 'amateursologirls',\n",
       " 'charlottecirclejerk',\n",
       " 'edmartists',\n",
       " 'thedeadweight',\n",
       " 'minttea',\n",
       " 'FreeHugsGaming',\n",
       " 'lukjad007',\n",
       " 'openonline',\n",
       " 'faloseit',\n",
       " 'Melee',\n",
       " 'Sexy3DAnimation',\n",
       " 'popcult',\n",
       " 'MTALI_Summer_2011',\n",
       " 'haiku_accounts',\n",
       " 'Jumpstyle',\n",
       " 'tonguetwisters',\n",
       " 'HaydenBrooke',\n",
       " 'BBCLuther',\n",
       " 'amipopularyet',\n",
       " 'Blythewood',\n",
       " 'upcycling',\n",
       " 'cultofharrietpotter',\n",
       " 'rasplex',\n",
       " 'opiatescirclejerk',\n",
       " 'divergentRP',\n",
       " 'arumba07',\n",
       " 'SexyWallpapers',\n",
       " 'OccupyHarrisburg',\n",
       " 'Remyanity',\n",
       " 'MaesterAlliance',\n",
       " 'EasternPhilosophy',\n",
       " 'ProjectBC',\n",
       " 'BattleSim',\n",
       " 'bys',\n",
       " 'visalia',\n",
       " 'amarillohigh',\n",
       " 'RPGinspiration',\n",
       " 'YesSheSquats',\n",
       " 'AmateurFilmmakers',\n",
       " 'shitreddithides',\n",
       " 'Owego',\n",
       " 'kanecirclejerk',\n",
       " 'GentlemannEstate',\n",
       " 'blackpanther',\n",
       " 'NHLStreams',\n",
       " 'MEFetishism',\n",
       " 'jesuschristreddit',\n",
       " 'tarantinogifs',\n",
       " 'CivWorldPowers',\n",
       " 'BrockTurnerInnocent',\n",
       " 'Grimes',\n",
       " 'Virtual_Reality',\n",
       " 'cumontongue',\n",
       " 'weeklyplanetpodcast',\n",
       " 'paydaymeta',\n",
       " 'Activeshows',\n",
       " 'medicare',\n",
       " 'LDSMLPERSOFND',\n",
       " 'dentonbuysell',\n",
       " 'carlworldproblems',\n",
       " 'uofrjapanstudies',\n",
       " 'thecollagist',\n",
       " 'breakingbadbanter',\n",
       " 'jailb8',\n",
       " 'BookWallpapers',\n",
       " 'traderjoes',\n",
       " 'deadrising',\n",
       " 'Heaven',\n",
       " 'Loras',\n",
       " 'boxgap',\n",
       " 'XtortionBear',\n",
       " 'CyYoung',\n",
       " 'Maddox',\n",
       " 'BulletHat',\n",
       " 'ScaredMeAsAKid',\n",
       " 'UCSB4Sale',\n",
       " 'TFIOS',\n",
       " 'MenOfAww',\n",
       " 'Do_Not_Do_It',\n",
       " 'zipzip',\n",
       " 'andrewgarfield',\n",
       " 'CivcraftValkyria',\n",
       " 'DaviksEstate',\n",
       " 'Girlsinjerseys',\n",
       " 'biting',\n",
       " 'treesrecipes',\n",
       " 'sabayon',\n",
       " 'pulpheroes',\n",
       " 'marshall2pro4u',\n",
       " 'mnatives',\n",
       " 'baseballnsfw',\n",
       " 'Amazing',\n",
       " 'ActualEagles',\n",
       " 'killrmusic',\n",
       " 'JennyPoussin',\n",
       " 'atheistmountaindew',\n",
       " 'amateurvids',\n",
       " 'ketchupwithlegs',\n",
       " 'elevate',\n",
       " 'FIFA_Fantasy',\n",
       " 'Pickles',\n",
       " 'traditionalist',\n",
       " 'explainlikeimcaveman',\n",
       " 'JGLGiraffes',\n",
       " 'poundacheez',\n",
       " 'guiltypleasures',\n",
       " 'muhfreedoms',\n",
       " 'Prattville',\n",
       " 'TNVaporEnthusiasts',\n",
       " 'dodgedart',\n",
       " 'StarCraftEnts',\n",
       " 'gastricsleeve',\n",
       " 'StatusKuo',\n",
       " 'globo',\n",
       " 'Bunnionaires',\n",
       " 'teknologi',\n",
       " 'SkypeShows',\n",
       " 'fiveSeveN_',\n",
       " 'acceptbitcoin',\n",
       " 'racialdiscussion',\n",
       " 'wowMentoringProgram',\n",
       " 'Tindie_Sellers',\n",
       " 'LeagueOfLovers',\n",
       " 'dc3591awesome',\n",
       " 'IvoryTower',\n",
       " 'live_newswire',\n",
       " 'ScrollsLL',\n",
       " 'LookWhatIPhotoshopped',\n",
       " 'messwithcaptionbot',\n",
       " 'shittylifeadvice',\n",
       " 'Eye_Twitching',\n",
       " 'Silverstein',\n",
       " 'CircPPSjerk',\n",
       " 'MetaLog',\n",
       " 'gmg',\n",
       " 'infinitesimal',\n",
       " 'VIdeoGameFans',\n",
       " 'BronyHateRebooted',\n",
       " 'HistoricalPowers',\n",
       " 'Biohackers',\n",
       " 'ThisWarofMine',\n",
       " 'TheoryPoE',\n",
       " 'nopost',\n",
       " 'Cataloniacoin',\n",
       " 'TextGaming',\n",
       " 'ShittySignaturePorn',\n",
       " 'AntiBrian',\n",
       " 'Novacoin',\n",
       " 'LSpolicecrew',\n",
       " 'homemediaserver',\n",
       " 'TEFLBlacklist',\n",
       " 'hotornot',\n",
       " 'Moosters',\n",
       " 'Planetside2Trolls',\n",
       " 'app317',\n",
       " 'TestAncora',\n",
       " 'toaruos',\n",
       " 'chickswearingchucks',\n",
       " 'goofynups',\n",
       " 'immortality_stratagem',\n",
       " 'BinaryMatrixPro',\n",
       " 'WhyMariosunsAnIdiot',\n",
       " 'Agawam',\n",
       " 'Iwishiwaslucky',\n",
       " 'AUSMUN2013',\n",
       " 'forgottenwebsites',\n",
       " 'DogeTrader',\n",
       " 'Mainz',\n",
       " 'ELEVISION',\n",
       " 'breakingbigbird',\n",
       " 'nolanlookshotincapes',\n",
       " 'Rule19',\n",
       " 'mazacoin',\n",
       " 'isshuukan',\n",
       " 'zombiegamers',\n",
       " 'UploadMinecraft',\n",
       " 'DFArt',\n",
       " 'VictoriaBCwaiterRant',\n",
       " 'greendress',\n",
       " 'SexTapes',\n",
       " 'Theotherotherground',\n",
       " 'BusinessTurtle',\n",
       " 'SydneyRoosters',\n",
       " 'meowcats',\n",
       " 'mroutCopyPasta',\n",
       " 'WTTF',\n",
       " 'masturbating',\n",
       " 'Leaky',\n",
       " 'BandstraBeard',\n",
       " 'InsertCoin',\n",
       " 'NCIXTechTips',\n",
       " 'TempEventCommitteeLOM',\n",
       " 'PNWlongboarding',\n",
       " 'BrisbaneMTB',\n",
       " 'Naestved',\n",
       " 'MyPrivateLinks',\n",
       " 'nsfwhq',\n",
       " 'GetGems',\n",
       " 'wsucomm111a',\n",
       " 'UTSAbooks',\n",
       " 'catsincolumbus',\n",
       " 'ilikeponiesandemotes',\n",
       " 'wildcards',\n",
       " 'EITMLYD',\n",
       " 'evanstonil',\n",
       " 'justdrawsomething',\n",
       " 'brianturner',\n",
       " 'OnlineFitness',\n",
       " 'andrewkang',\n",
       " 'mixedgirls',\n",
       " 'missingpets',\n",
       " 'AdagioDazzle',\n",
       " 'CSR',\n",
       " 'peopleshistory',\n",
       " 'GolfitCC',\n",
       " 'OfficialNoFapWar',\n",
       " 'steamsucks',\n",
       " 'CharlieParker',\n",
       " 'Xegsa',\n",
       " 'aerialtracks',\n",
       " 'thefinalmeowmeowmeow',\n",
       " 'fuckdominick',\n",
       " 'FenwaySportsGroup',\n",
       " 'sassygifs',\n",
       " 'highlining',\n",
       " 'muttonchopeth',\n",
       " 'trynagetahandbeezy',\n",
       " 'ponywalls',\n",
       " 'SISCASMTS',\n",
       " 'SavageNation',\n",
       " 'spactard',\n",
       " 'FTMsGoneWild',\n",
       " 'minecrafthungerstream',\n",
       " 'BlackLiterature',\n",
       " 'Spaghetti5Ever',\n",
       " '4fingers',\n",
       " 'FrozenFun',\n",
       " 'gunkbros',\n",
       " 'coolinternettricks',\n",
       " 'hci',\n",
       " 'TDPWriting',\n",
       " 'Silk',\n",
       " '2QB',\n",
       " 'battlebane_',\n",
       " 'SnowflakeMasterRace',\n",
       " 'TheLittleThingsGaming',\n",
       " '1970s',\n",
       " 'brackets',\n",
       " 'MonsterFishKeepers',\n",
       " 'BaguetteShame',\n",
       " 'Cosmere',\n",
       " 'MinotaurMadness',\n",
       " 'GemsofWar',\n",
       " 'iCloud',\n",
       " 'DailyFootage',\n",
       " 'YakusokuNoNeverland',\n",
       " 'SoItBegins',\n",
       " 'AlabamEnts',\n",
       " 'psychoclash',\n",
       " 'rSmite',\n",
       " 'guernsey',\n",
       " 'shitSRDIRCsaysSucks',\n",
       " 'pureketo',\n",
       " 'canadianstartups',\n",
       " 'evemissions',\n",
       " 'Bookie',\n",
       " 'TBIsurvivors',\n",
       " 'nobodycaresaboutme',\n",
       " 'BlogNetworking',\n",
       " 'fatpeopleinscooters',\n",
       " 'Petroleum',\n",
       " 'TheJunction',\n",
       " 'RewardSiteGuides',\n",
       " 'hCGifs',\n",
       " 'GoneOrgasm',\n",
       " 'JustCrimsonThings',\n",
       " 'Jetpackfighter',\n",
       " 'SuchFlex',\n",
       " 'TomScott',\n",
       " 'ROKCFIT',\n",
       " 'djiinspire1',\n",
       " 'slowmovement',\n",
       " 'TwoXPenpals',\n",
       " 'Positivity',\n",
       " 'synapsevr',\n",
       " 'truthDPRK',\n",
       " 'Gamesalad',\n",
       " 'Shrinkme',\n",
       " 'darkskingirls',\n",
       " 'WrestlingReactions',\n",
       " 'mgo3',\n",
       " 'f0k',\n",
       " 'NewbieNation',\n",
       " 'Allison_Williams',\n",
       " 'marketZ',\n",
       " 'freebooting',\n",
       " 'TagProworldcupgermany',\n",
       " 'SummerCoinV2',\n",
       " 'savetheconstitution',\n",
       " 'SinclairStuff',\n",
       " 'ChinaStories',\n",
       " 'CommonGroundMovement',\n",
       " '9CB9D65F54ED858A',\n",
       " 'isp',\n",
       " 'OneMoreDay',\n",
       " 'how_to_meet_girls',\n",
       " 'Duoqueuelol',\n",
       " 'tunnelsteaks',\n",
       " 'puzzleroom',\n",
       " 'dogsledracing',\n",
       " 'Sodatowers',\n",
       " 'LeicesterTigers',\n",
       " 'NexusCorps',\n",
       " 'redditmeta',\n",
       " 'SORG1_IC4',\n",
       " 'Vemund',\n",
       " 'EverydayMuslims',\n",
       " 'amateurradiotest',\n",
       " 'Umzug',\n",
       " 'PeterHitchens',\n",
       " 'TacticalTwerking',\n",
       " 'tsumea',\n",
       " 'drumsbassguitar',\n",
       " 'TalesFromTheMortuary',\n",
       " 'doguessing',\n",
       " 'BostonRSS',\n",
       " 'southyorkshire',\n",
       " 'mikakunin',\n",
       " 'GROUPCoin',\n",
       " 'deftly',\n",
       " 'GetFairShare',\n",
       " 'SanJac',\n",
       " 'askhistorian',\n",
       " 'thegameofgames',\n",
       " 'PlayBall',\n",
       " 'DarkSouls2League',\n",
       " 'thriftstorefinds',\n",
       " 'pvris',\n",
       " 'ceylon',\n",
       " 'Pittakionophobia',\n",
       " 'Crunkle',\n",
       " 'HeavyGearAssault',\n",
       " 'pitbikes',\n",
       " 'pireThoughts',\n",
       " 'lizardworks',\n",
       " 'GenY',\n",
       " 'notrevolutionary',\n",
       " 'propellerporn',\n",
       " 'Dagenssill',\n",
       " 'failtexts',\n",
       " 'Eatersofbees',\n",
       " 'shesontop',\n",
       " 'telseccompolicy',\n",
       " 'NBLCreative',\n",
       " 'Edg3',\n",
       " 'ShitTedCruzSays',\n",
       " 'nintendocanada',\n",
       " 'nothingdoesanything',\n",
       " 'watermelonfans',\n",
       " 'NuclearThroneLeague',\n",
       " 'ccosentino',\n",
       " 'approachio',\n",
       " 'ehtc',\n",
       " 'CdnLiberalParty',\n",
       " '829',\n",
       " 'MangledShortLinks',\n",
       " 'lolyou',\n",
       " 'Hypnagogia',\n",
       " 'ossifiedevidenc',\n",
       " 'johndelancie',\n",
       " 'phoenixgg2',\n",
       " 'telnet',\n",
       " 'feddhurs',\n",
       " 'letters2judges',\n",
       " 'traveling',\n",
       " 'CGWhiskeyReviews',\n",
       " 'GameChats',\n",
       " 'ausbus',\n",
       " 'MildlyUncomfortable',\n",
       " 'corruptscience',\n",
       " 'Danmakufu',\n",
       " 'JessicaChastain',\n",
       " 'morontrepreneurs',\n",
       " 'whitepeopleproblems',\n",
       " 'PunishAndEnslave',\n",
       " 'nickofferman',\n",
       " 'EthiopiaPics',\n",
       " 'Suspicious0bservers',\n",
       " 'teacherselfies',\n",
       " 'CSBCamp',\n",
       " 'toddlerproblems',\n",
       " 'DumbShitSaid',\n",
       " 'MotorcyclesVancouver',\n",
       " 'DontBelieveMe',\n",
       " 'Palermo',\n",
       " 'gentlefemdom',\n",
       " 'TechLoL',\n",
       " 'All_the_X_squared',\n",
       " 'blockchaintech',\n",
       " 'mohawk_ui',\n",
       " 'CETest',\n",
       " 'franklincountyga',\n",
       " 'LGBTraveler',\n",
       " 'SpermBank',\n",
       " 'accountabilitygroups',\n",
       " 'fingerblasting',\n",
       " 'sextips',\n",
       " 'video_nsfw',\n",
       " 'BeastSouls',\n",
       " 'Britishretail',\n",
       " 'geoglassing',\n",
       " 'survivorspoilers',\n",
       " 'ModelAusSenate',\n",
       " 'niceamiibo',\n",
       " 'DogeMiningHelp',\n",
       " 'ATandT',\n",
       " 'cetuscoin',\n",
       " 'Imagineering',\n",
       " 'TheoryOfGreddit',\n",
       " 'fluffythings',\n",
       " 'gethype',\n",
       " 'SillyMCbanreasons',\n",
       " 'cigd',\n",
       " 'WinnipegCirclejerk',\n",
       " 'A302015Discussion',\n",
       " 'FuckYouImBatman',\n",
       " 'Leadwerks',\n",
       " 'FarlightExplorers',\n",
       " 'TeamProCoin',\n",
       " 'dubiski',\n",
       " 'Showmasters',\n",
       " 'notevenasubreddit',\n",
       " 'shitmormonssay',\n",
       " 'Sitting',\n",
       " 'Regularthoughts',\n",
       " 'ShittyAskHarryPotter',\n",
       " 'lonelyheartbeats',\n",
       " 'monte',\n",
       " 'lewd_phone_wallpapers',\n",
       " 'BoobiesGoneWild',\n",
       " 'trpFinance',\n",
       " 'CarrotsCarrotsCarrots',\n",
       " 'TDRExtra',\n",
       " 'lavendertest',\n",
       " 'EUReferendum',\n",
       " 'Unicums',\n",
       " 'HSDrama',\n",
       " 'TailLovers',\n",
       " 'nodop',\n",
       " 'RetardedReddit',\n",
       " 'hearthpacks',\n",
       " 'SNH48',\n",
       " 'abcqwerty124',\n",
       " 'spaceniglets',\n",
       " 'Brynhildr',\n",
       " 'regexgolfing',\n",
       " 'Sabin357_RPG',\n",
       " 'TheBibleReloaded',\n",
       " 'r6k',\n",
       " 'BBQSauce',\n",
       " 'TheLunchLadyCalled',\n",
       " 'shittyteardowns',\n",
       " 'pokemongiveawaygalore',\n",
       " 'kissablefaces',\n",
       " 'GirlsMirinGirls',\n",
       " 'SkyShield',\n",
       " 'TheChatotGames',\n",
       " 'Pikistikman',\n",
       " 'ShowingOffInPublic',\n",
       " 'CourtneyTailor',\n",
       " 'whydididothis',\n",
       " 'RockStarEditor',\n",
       " 'iosgame_ja',\n",
       " 'hellowork',\n",
       " 'GoogleRank',\n",
       " 'CrackerPorn',\n",
       " 'PMI',\n",
       " 'linuxladies',\n",
       " 'PCOverlord',\n",
       " 'Ideasandconcepts',\n",
       " 'CollabMultiverse',\n",
       " 'hoodriver',\n",
       " 'herasplayground',\n",
       " 'thelaggingnation',\n",
       " 'DenverGameSwap',\n",
       " 'TheSlothKingdom',\n",
       " 'RedKadathreviews',\n",
       " 'UFABC',\n",
       " 'lisaholm',\n",
       " 'todaysstupidproblem',\n",
       " 'asawhiteperson',\n",
       " 'Razlyk_The_Great',\n",
       " 'Earbuds',\n",
       " 'TeamCLG',\n",
       " 'FazyRektOnceAgain',\n",
       " 'workingfromhome',\n",
       " 'Max247',\n",
       " 'MWHS',\n",
       " 'TheHellsGate',\n",
       " '160thsor',\n",
       " 'HookerBloopers',\n",
       " 'cem1790',\n",
       " 'Remedy',\n",
       " 'RunningSimulator',\n",
       " 'nightmarepeople',\n",
       " 'Member_Berries',\n",
       " 'ImagesOfSingapore',\n",
       " 'TF2Youngsters',\n",
       " 'ATPfm',\n",
       " 'kurtrosenwinkel',\n",
       " 'LovingIt',\n",
       " 'Victinisclaw',\n",
       " 'theendoftheworldrpg',\n",
       " 'bellahadid',\n",
       " 'lesbianstories',\n",
       " 'RPzone',\n",
       " 'AppenWAH',\n",
       " 'KawaiiCraft',\n",
       " 'blissfest',\n",
       " 'Whyisthishappening',\n",
       " 'Delaware4Sanders',\n",
       " 'Bucks',\n",
       " 'Kinkyfur',\n",
       " 'Sword',\n",
       " 'MichelsenComputersNZ',\n",
       " 'AhriLoL',\n",
       " 'avatarmasternation',\n",
       " 'NewtonAbbot',\n",
       " 'ExileGuild',\n",
       " 'arguments',\n",
       " 'Stydia',\n",
       " 'bandagedresses',\n",
       " 'UHCNation',\n",
       " 'CowButtholeViolators',\n",
       " 'JeffTuel',\n",
       " 'starcitizen_TN',\n",
       " 'fightingamphibians',\n",
       " 'ketofreestyle',\n",
       " 'prorugby',\n",
       " 'JetSellers',\n",
       " 'UpsideDownR',\n",
       " 'CampJoelwhyrock',\n",
       " 'praisezenith4216',\n",
       " 'OldManDog',\n",
       " 'IdeasForSequels',\n",
       " 'so_sweet_like_anise',\n",
       " 'OurPresident',\n",
       " 'kachow',\n",
       " 'Neonkisses',\n",
       " 'ReZeroHentai',\n",
       " 'mydnmreviewacct',\n",
       " 'ShatterDeals',\n",
       " 'muorigins',\n",
       " 'UKdocumentaries',\n",
       " 'TotalDramaYourChoice',\n",
       " 'valentinanappi',\n",
       " 'AGameOfArk',\n",
       " 'stonersongs',\n",
       " 'TheProgressShow',\n",
       " 'Minyakbimoli',\n",
       " 'internetsuperpac',\n",
       " 'LGAlly',\n",
       " 'COM98',\n",
       " 'crushcrush',\n",
       " 'theodd1sout',\n",
       " 'sp4zie',\n",
       " 'photoonstage',\n",
       " 'Dinkelturd',\n",
       " 'athree',\n",
       " 'Odewatch',\n",
       " 'sometimeswhatidois',\n",
       " 'axiouscraft',\n",
       " 'BeautyGuruChatter',\n",
       " 'GoneWild_Curvy_Teens',\n",
       " 'Allthewaythrough',\n",
       " 'RocketLeagueMarket',\n",
       " 'Grumpamon',\n",
       " 'hanna',\n",
       " 'ImKickAss',\n",
       " 'Procrastionation',\n",
       " 'Notapsyduck',\n",
       " 'otherpeoplelaughing',\n",
       " 'MarioBaseball',\n",
       " 'amazfit',\n",
       " 'cirlejekr',\n",
       " 'Schoolhouse',\n",
       " 'BernersForStein',\n",
       " 'Crossdress',\n",
       " 'InsomniacGames',\n",
       " 'lufia',\n",
       " 'stuffnottodo',\n",
       " 'iradiostations',\n",
       " 'modresponses',\n",
       " 'MNLoLPubstomp',\n",
       " 'robjonespartyzone',\n",
       " 'historicalmodelusgov',\n",
       " 'runningForOffice_GA',\n",
       " 'MSU_Spartans',\n",
       " 'ModelIraq',\n",
       " 'lesbianOral',\n",
       " 'Csstestting',\n",
       " 'ihatetheusa',\n",
       " 'freepsd',\n",
       " 'choicepoint',\n",
       " 'KronosChat',\n",
       " 'MarioSonicOlympics',\n",
       " 'Diepmutations',\n",
       " 'TelegramChannels',\n",
       " 'PensacolaMemes',\n",
       " 'BaseballClubHouse',\n",
       " 'alybt',\n",
       " 'Mynameisname',\n",
       " 'PokemonAdvertising',\n",
       " 'HentaiMusicVideos',\n",
       " 'smithsonvalley',\n",
       " 'Ahu',\n",
       " 'NewYorker',\n",
       " 'AnalFood',\n",
       " 'italypremium',\n",
       " 'Notakeonlythrow',\n",
       " 'RevDem',\n",
       " 'ClashofClansAccounts',\n",
       " 'ThaWlkingDed',\n",
       " 'what2code',\n",
       " 'compressioncrops',\n",
       " 'KettleBot',\n",
       " 'overwatch_fanart',\n",
       " 'ClintonInvestigation',\n",
       " 'ArSuspovob_pblrpadt_',\n",
       " 'thsprom',\n",
       " 'TheStrangeLands',\n",
       " 'DnDFetuses',\n",
       " 'strikeforcethunder',\n",
       " 'ModelTimHortons',\n",
       " 'cadeteam_CrineaiE_ed',\n",
       " 'Fajoutgoischnhtgcihe',\n",
       " 'PaganCrin',\n",
       " 'Mariitofy_adS4inon',\n",
       " 'beepboopunite',\n",
       " 'FollowersofToast',\n",
       " 'Pimple_Meat',\n",
       " 'TashaReign',\n",
       " 'PokemonGoVentura',\n",
       " 'BigForeheads',\n",
       " 'JockCircleJerk',\n",
       " 'CommunityTap',\n",
       " 'BartClaessen',\n",
       " 'Homunculi',\n",
       " 'BeeMusic',\n",
       " 'KaizoMarioMaker',\n",
       " 'GodSaveTheBants',\n",
       " 'Jumpingoffofstuff',\n",
       " 'Pokedoption',\n",
       " 'elocution',\n",
       " 'ProjektHoerspiel',\n",
       " 'TVandFilmCliches',\n",
       " 'vgversus',\n",
       " 'WorkingAtHome',\n",
       " 'Hotwivesprep',\n",
       " 'peekaboo',\n",
       " 'FakePorner_Trans',\n",
       " 'BTYWRESTLING',\n",
       " 'DDStrategies',\n",
       " 'CassandraConspiracy',\n",
       " 'LibertadFinancieraMX',\n",
       " 'YosemitePhotography',\n",
       " 'deer_hunting',\n",
       " 'NieceWaidhofer',\n",
       " 'elisandjohn',\n",
       " 'TeamAdviceMM',\n",
       " 'Notsalt',\n",
       " 'FinalFlutterLove',\n",
       " 'crazytinfoilhats',\n",
       " 'benfeild',\n",
       " 'cubeworldbanappeals',\n",
       " 'QuiGonJinnisawesome',\n",
       " 'FlyingSagittarius',\n",
       " 'truemccirclejerk',\n",
       " 'TheGuildOfStupidity',\n",
       " 'Frontpageforall',\n",
       " 'chubbit',\n",
       " 'ineedfriends',\n",
       " 'christiancool',\n",
       " 'IrelandPodcasting',\n",
       " 'qype',\n",
       " 'dantmemes',\n",
       " 'singlikeyoda',\n",
       " 'WWIJAA',\n",
       " 'hdadviceanimals',\n",
       " 'Ember',\n",
       " 'MarioMakerplayers',\n",
       " 'althistorytimelines',\n",
       " 'AllFashion',\n",
       " 'HouseManderly',\n",
       " 'zingg',\n",
       " 'StreetFighterTech',\n",
       " 'postgres',\n",
       " 'saturationporn',\n",
       " 'TripLikeAnimals',\n",
       " 'ReggaeToronto',\n",
       " 'dirtsmoke',\n",
       " 'KieranIsAQuadrupleFag',\n",
       " 'sexaddicts',\n",
       " 'keralapics',\n",
       " 'csgotradingofficial',\n",
       " 'Starcraft2arcade',\n",
       " 'wittywalrus1',\n",
       " 'LetsDesignAGame',\n",
       " 'thenumber42',\n",
       " 'thomascott',\n",
       " 'Gruul',\n",
       " 'Butternutsquash',\n",
       " 'badegyptology',\n",
       " 'PressureCookerRecipes',\n",
       " 'Orangechat',\n",
       " 'VortexUnit',\n",
       " 'RejsbyEfterskole',\n",
       " 'RapBeats',\n",
       " 'morehumanthanhuman',\n",
       " 'RebelRed',\n",
       " 'PriveeMe',\n",
       " 'poisreviews',\n",
       " 'BrasOnTitsOut',\n",
       " 'Pioneer1936',\n",
       " 'WrestlingPPVPlans',\n",
       " 'realadvice',\n",
       " 'csscrim',\n",
       " 'blakeschin',\n",
       " 'SchadenfreudeNews',\n",
       " 'BoostLife',\n",
       " 'makeupbeauty',\n",
       " 'werrttyyu',\n",
       " 'Writersfever',\n",
       " 'DisplayPortMasterRace',\n",
       " 'myhobby',\n",
       " 'pokemongo_ftmeade',\n",
       " 'CivilizedSomeDay',\n",
       " 'ResearchedHema',\n",
       " 'Olympussisters',\n",
       " 'SleepDeprived',\n",
       " 'DanIsDumb',\n",
       " 'idiotsvsobjects',\n",
       " 'iwillnotstopdrinking',\n",
       " 'Flatworld',\n",
       " 'EncouragingNews',\n",
       " 'MaryJane420',\n",
       " 'getofmylawn',\n",
       " 'reeeeeeeeeeeeeeeeeee',\n",
       " 'misleadingpictures',\n",
       " 'CMLMagicDiary',\n",
       " 'Pulidofanclub',\n",
       " 'awenime',\n",
       " 'wto',\n",
       " 'ForwardsFromLeRedPill',\n",
       " 'Ratajkowski_nude',\n",
       " 'fucknoturnunstoned',\n",
       " 'LlamarrWilson',\n",
       " 'LouisCatchphrases',\n",
       " 'VRTesting',\n",
       " 'BattleCupDota2',\n",
       " 'Volume',\n",
       " 'DoppelFrankers',\n",
       " 'AMDRadeon',\n",
       " 'HeartsOfSteel',\n",
       " 'yosenjus',\n",
       " 'Whiteshit',\n",
       " 'TheNextGenGames',\n",
       " 'rpg2knet',\n",
       " 'VeryFunny_Pics',\n",
       " 'OneTrueIndex',\n",
       " 'storyhour',\n",
       " 'Corre',\n",
       " 'iPhoneographie',\n",
       " 'sgla',\n",
       " 'interestingplates',\n",
       " 'flushrush',\n",
       " 'Daf_and_Aus',\n",
       " 'JamesAlex',\n",
       " 'YourPokemonFavorites',\n",
       " 'lonnez',\n",
       " 'carfanatiks',\n",
       " 'ARStoryTime',\n",
       " 'starwarseurp',\n",
       " 'RandomImgurImages',\n",
       " 'Middle_reddit',\n",
       " 'holdmyeggnog',\n",
       " 'BeautyandtheBeastFilm',\n",
       " 'TSV1860',\n",
       " 'rmhsschoolmd',\n",
       " 'MEAction',\n",
       " '4chong',\n",
       " 'reptileilluminati',\n",
       " 'LudoKressh',\n",
       " 'MapsofAmerica',\n",
       " 'HolmvsdeRandamieLivez',\n",
       " '12monthloan',\n",
       " 'TheHolyQuran',\n",
       " 'Politicalmoderates',\n",
       " 'allhaileris',\n",
       " 'NSPGame',\n",
       " 'Kenyans',\n",
       " 'NewDelhiMusicCo',\n",
       " 'BreakingNflNews',\n",
       " 'DiamondVeneers',\n",
       " 'DeepRoy',\n",
       " 'SlimJim',\n",
       " 'Aussienerdfighters',\n",
       " 'Conbananacy',\n",
       " 'streakrunning',\n",
       " 'McMenno',\n",
       " 'WriterJim',\n",
       " 'threux',\n",
       " 'LaidbackLoL',\n",
       " 'anb',\n",
       " 'waterparkpics',\n",
       " 'pushback',\n",
       " 'NoMansSeed',\n",
       " 'pagecloud',\n",
       " 'gentlemanarguments',\n",
       " 'Ikea_Monkey',\n",
       " 'CMUQ',\n",
       " 'HPAnerf',\n",
       " 'NerdyChallenge',\n",
       " 'catswatchingyoufap',\n",
       " 'TheWizardsLounge',\n",
       " 'Skorne',\n",
       " 'teamwindmill',\n",
       " 'jeffhasagf',\n",
       " 'Klingism',\n",
       " 'StarsInShadow',\n",
       " 'ShatteredThrone',\n",
       " 'Modtale',\n",
       " 'irreligious',\n",
       " 'RepulicOTueAllReboted',\n",
       " 'ruarks',\n",
       " 'Victoria_BC',\n",
       " 'benandbryan',\n",
       " 'DeathsShadow',\n",
       " 'StarterKits',\n",
       " 'AltRight_Is_Alright',\n",
       " 'xchromosome',\n",
       " 'EmboldenTheY',\n",
       " 'RedditAPICrawlerTest',\n",
       " 'wifflehome',\n",
       " 'unioncalmers',\n",
       " 'SoupKitchen',\n",
       " 'USElectionsITA',\n",
       " 'FuckedUpBooks',\n",
       " 'ImagesOfMuhammad',\n",
       " 'santacruzbo',\n",
       " 'Dare4Gold',\n",
       " 'WritingJobBoard',\n",
       " 'elliotsbirthday',\n",
       " 'NONPRIVATECIRCLEJERK',\n",
       " 'Keithellison',\n",
       " 'unpresidented',\n",
       " 'ThirdParties',\n",
       " 'YOLOBROSCLUBOFSWAG',\n",
       " 'onion_omega',\n",
       " 'StephanieMoretti',\n",
       " 'flashyrebirth',\n",
       " 'Kyowtwo',\n",
       " 'BeefCAKEAbbey',\n",
       " 'whatsupwithyou',\n",
       " 'TheINDogProject',\n",
       " 'WestConnex',\n",
       " 'gameexchange',\n",
       " 'HasItLeaked',\n",
       " 'fixingdemocracy',\n",
       " 'sportsrumors',\n",
       " 'fungalbiology',\n",
       " 'EMW',\n",
       " 'RepairMyPC',\n",
       " 'the_lobster',\n",
       " 'skilltroll',\n",
       " 'novapbs',\n",
       " 'auostriliotica',\n",
       " 'ifoundamickey',\n",
       " 'geniousshit',\n",
       " 'GoetzIT',\n",
       " 'a7s',\n",
       " 'drawsomethingpics',\n",
       " 'jaielogaming',\n",
       " 'Shenmue_jp',\n",
       " 'willthan10',\n",
       " 'sofloantonio',\n",
       " 'Fireplaces_Stoves',\n",
       " 'Weirdsciencedccomics',\n",
       " 'brotherhoodofsloth',\n",
       " 'windia',\n",
       " ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits = []\n",
    "i = 0\n",
    "for row in text_subreddits.rdd.collect():\n",
    "    subreddit = row['subreddit']\n",
    "#     print(subreddit)\n",
    "    subreddits.append(subreddit)\n",
    "    i += 1\n",
    "#     if i == 20:\n",
    "#         break\n",
    "\n",
    "subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anime\n",
      "querying...\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 973 in stage 9.0 failed 1 times, most recent failure: Lost task 973.0 in stage 9.0 (TID 9324, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 154814 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a3998dd8bb23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mwords_subreddit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_subreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mwords_subreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.4-bin-hadoop2.7/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.4-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark-2.4.4-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 973 in stage 9.0 failed 1 times, most recent failure: Lost task 973.0 in stage 9.0 (TID 9324, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 154814 ms\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "words = []\n",
    "for subreddit in subreddits:\n",
    "    print(subreddit)\n",
    "    \n",
    "    query = '''\n",
    "    SELECT\n",
    "      subreddit, \n",
    "      body\n",
    "    FROM\n",
    "      reddit_data\n",
    "    WHERE\n",
    "      subreddit = \"%s\"\n",
    "      AND\n",
    "      body != \"[deleted]\"\n",
    "    ''' % (subreddit)\n",
    "\n",
    "    print(\"querying...\")\n",
    "    text_subreddit = spark.sql(query)\n",
    "    \n",
    "    words_subreddit = []\n",
    "    for row in text_subreddit.rdd.collect():\n",
    "        for word in row[\"body\"].split(\" \"):\n",
    "            words_subreddit.append(word)\n",
    "    \n",
    "    words.append(words_subreddit)\n",
    "    \n",
    "    i += 1\n",
    "#     if i == 2:\n",
    "#         break\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yeah,',\n",
       " \"it's\",\n",
       " 'like',\n",
       " 'saying',\n",
       " '\"women',\n",
       " 'are',\n",
       " 'better',\n",
       " 'at',\n",
       " 'getting',\n",
       " 'custody',\n",
       " 'of',\n",
       " 'their',\n",
       " 'children',\n",
       " 'in',\n",
       " 'court\"',\n",
       " 'and',\n",
       " 'bragging',\n",
       " 'about',\n",
       " 'it.',\n",
       " 'You',\n",
       " 'mean',\n",
       " 'carcinogenic',\n",
       " 'gasses?',\n",
       " '\\n\\nSorry',\n",
       " 'for',\n",
       " 'the',\n",
       " 'pedantry,',\n",
       " 'I',\n",
       " 'do',\n",
       " 'agree',\n",
       " 'though.',\n",
       " \"They're\",\n",
       " 'great',\n",
       " 'jobs',\n",
       " 'for',\n",
       " 'young',\n",
       " 'people.',\n",
       " 'Not',\n",
       " 'so',\n",
       " 'much',\n",
       " 'family',\n",
       " 'people.',\n",
       " 'A',\n",
       " 'voice',\n",
       " 'for',\n",
       " 'men',\n",
       " 'radio',\n",
       " 'and',\n",
       " 'www.register-her.com',\n",
       " 'whats',\n",
       " 'the',\n",
       " 'context',\n",
       " 'here?',\n",
       " 'I',\n",
       " 'think',\n",
       " 'we',\n",
       " 'all',\n",
       " 'agree',\n",
       " 'quotas',\n",
       " 'are',\n",
       " 'dumb,',\n",
       " 'when',\n",
       " 'they',\n",
       " 'are',\n",
       " 'brought',\n",
       " 'up',\n",
       " '(outside)',\n",
       " 'we',\n",
       " 'should',\n",
       " 'present',\n",
       " 'something',\n",
       " 'like',\n",
       " 'a',\n",
       " '[blind',\n",
       " 'audition](http://en.wikipedia.org/wiki/Blind_audition),',\n",
       " 'where',\n",
       " 'the',\n",
       " 'potential',\n",
       " 'for',\n",
       " 'gender',\n",
       " 'bias',\n",
       " 'is',\n",
       " 'removed',\n",
       " 'or',\n",
       " 'minimized.',\n",
       " \"\\n\\nIt's\",\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'easier',\n",
       " 'fight',\n",
       " 'against',\n",
       " 'this',\n",
       " 'type',\n",
       " 'of',\n",
       " 'sexism',\n",
       " 'when',\n",
       " 'we',\n",
       " 'force',\n",
       " 'the',\n",
       " 'other',\n",
       " 'side',\n",
       " 'to',\n",
       " 'argue',\n",
       " 'against',\n",
       " 'a',\n",
       " 'meritocracy.',\n",
       " 'we',\n",
       " 'have',\n",
       " 'a',\n",
       " 'policy',\n",
       " 'of',\n",
       " 'no',\n",
       " 'Sandals',\n",
       " 'or',\n",
       " 'flip',\n",
       " 'flops.....\\n\\neveryday',\n",
       " 'I',\n",
       " 'see',\n",
       " 'women',\n",
       " 'at',\n",
       " 'work',\n",
       " 'wearing',\n",
       " 'sandals',\n",
       " 'or',\n",
       " 'flip',\n",
       " 'flops.\\n\\nPetty....',\n",
       " '',\n",
       " 'but',\n",
       " 'I',\n",
       " 'also',\n",
       " 'saw',\n",
       " 'one',\n",
       " 'of',\n",
       " 'my',\n",
       " 'male',\n",
       " 'co-workers',\n",
       " 'get',\n",
       " 'sent',\n",
       " 'home',\n",
       " 'to',\n",
       " '\"change\"',\n",
       " 'because',\n",
       " 'of',\n",
       " 'it.\\n\\n\\nalso,',\n",
       " '',\n",
       " 'a',\n",
       " 'buddy',\n",
       " 'of',\n",
       " 'mine',\n",
       " 'has',\n",
       " 'an',\n",
       " 'HR',\n",
       " 'crew',\n",
       " '(all',\n",
       " 'women)',\n",
       " 'that',\n",
       " 'gather',\n",
       " 'to',\n",
       " 'wear',\n",
       " 'shirts',\n",
       " 'saying,',\n",
       " '\"Men',\n",
       " 'are',\n",
       " 'pigs\".....',\n",
       " '',\n",
       " 'No',\n",
       " 'backlash.',\n",
       " '',\n",
       " \"I'm\",\n",
       " 'fairly',\n",
       " 'positive',\n",
       " 'if',\n",
       " 'he',\n",
       " 'wore',\n",
       " 'a',\n",
       " 'shirt',\n",
       " 'stating,',\n",
       " '\"Women',\n",
       " 'are',\n",
       " 'pigs\"',\n",
       " 'he',\n",
       " 'would',\n",
       " 'be',\n",
       " 'fired.',\n",
       " 'Which',\n",
       " 'is',\n",
       " 'why',\n",
       " 'the',\n",
       " \"Men's\",\n",
       " 'Rights',\n",
       " 'movement',\n",
       " 'must',\n",
       " 'address',\n",
       " 'the',\n",
       " 'issue',\n",
       " 'of',\n",
       " 'Government.',\n",
       " '...',\n",
       " 'what?',\n",
       " 'No.',\n",
       " \"It's\",\n",
       " 'called',\n",
       " \"Men's\",\n",
       " 'Rights,',\n",
       " 'not',\n",
       " 'White',\n",
       " \"Men's\",\n",
       " 'rights.',\n",
       " '&gt;Me',\n",
       " 'talking',\n",
       " 'about',\n",
       " 'you',\n",
       " 'specifically',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'how',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'about',\n",
       " 'men?\\n\\nWhen',\n",
       " 'my',\n",
       " 'comments',\n",
       " 'are',\n",
       " 'specifically',\n",
       " 'dealing',\n",
       " 'with',\n",
       " 'equality',\n",
       " 'and',\n",
       " 'how',\n",
       " 'such',\n",
       " 'issues',\n",
       " 'are',\n",
       " 'discussed,',\n",
       " 'and',\n",
       " 'you',\n",
       " 'disagree',\n",
       " 'with',\n",
       " 'those',\n",
       " 'assertions',\n",
       " 'which',\n",
       " 'favour',\n",
       " 'equality',\n",
       " 'for',\n",
       " 'men,',\n",
       " 'then',\n",
       " 'yes,',\n",
       " 'of',\n",
       " 'course',\n",
       " 'those',\n",
       " 'are',\n",
       " 'examples',\n",
       " 'of',\n",
       " 'how',\n",
       " 'you',\n",
       " 'feel',\n",
       " 'about',\n",
       " 'men.',\n",
       " 'The',\n",
       " 'comments',\n",
       " 'are',\n",
       " 'in',\n",
       " 'plain',\n",
       " 'sight.',\n",
       " 'Calling',\n",
       " 'bullshit',\n",
       " 'when',\n",
       " 'we',\n",
       " 'can',\n",
       " 'all',\n",
       " 'see',\n",
       " 'the',\n",
       " 'comments',\n",
       " 'is',\n",
       " 'laughable.',\n",
       " 'As',\n",
       " 'everyone',\n",
       " 'here',\n",
       " 'knows,',\n",
       " \"I've\",\n",
       " 'never',\n",
       " 'argued',\n",
       " 'for',\n",
       " 'anything',\n",
       " 'other',\n",
       " 'than',\n",
       " 'strict',\n",
       " 'equality.',\n",
       " 'Pretending',\n",
       " 'that',\n",
       " 'I',\n",
       " 'favour',\n",
       " 'male',\n",
       " 'privilege',\n",
       " 'is',\n",
       " 'ridiculous.\\n\\nSo,',\n",
       " 'in',\n",
       " 'the',\n",
       " 'eyes',\n",
       " 'of',\n",
       " 'everyone',\n",
       " 'here,',\n",
       " 'point',\n",
       " 'out',\n",
       " 'any',\n",
       " 'comments',\n",
       " 'in',\n",
       " 'my',\n",
       " 'history',\n",
       " 'which',\n",
       " 'show',\n",
       " 'me',\n",
       " '\"pushing',\n",
       " 'a',\n",
       " 'male',\n",
       " 'privilege',\n",
       " 'agenda\".',\n",
       " 'Surely',\n",
       " 'those',\n",
       " 'would',\n",
       " 'be',\n",
       " 'easy',\n",
       " 'to',\n",
       " 'find',\n",
       " 'if',\n",
       " 'you',\n",
       " \"weren't\",\n",
       " 'a',\n",
       " 'lying',\n",
       " 'ideologue',\n",
       " 'yourself?',\n",
       " 'True',\n",
       " 'enough,',\n",
       " 'point',\n",
       " 'taken.',\n",
       " 'no,',\n",
       " 'the',\n",
       " 'fact',\n",
       " 'that',\n",
       " 'he',\n",
       " 'essentially',\n",
       " 'accused',\n",
       " 'me',\n",
       " 'of',\n",
       " 'being',\n",
       " 'gay',\n",
       " 'isnt',\n",
       " 'that',\n",
       " 'like',\n",
       " 'labia',\n",
       " 'plasty?',\n",
       " 'some',\n",
       " 'people',\n",
       " 'choose',\n",
       " 'to',\n",
       " 'have',\n",
       " 'that',\n",
       " 'done',\n",
       " 'because',\n",
       " 'they',\n",
       " 'think',\n",
       " 'it',\n",
       " \"'looks\",\n",
       " \"better',\",\n",
       " 'isnt',\n",
       " 'as',\n",
       " 'fucked',\n",
       " 'up',\n",
       " 'as',\n",
       " 'chopping',\n",
       " 'off',\n",
       " 'the',\n",
       " 'clit',\n",
       " '(ouch)',\n",
       " '',\n",
       " \"I'm\",\n",
       " 'sorry',\n",
       " 'that',\n",
       " 'you',\n",
       " 'feel',\n",
       " 'that',\n",
       " 'way.',\n",
       " '',\n",
       " 'Here',\n",
       " 'is',\n",
       " 'the',\n",
       " '2nd',\n",
       " 'response',\n",
       " 'to',\n",
       " 'that',\n",
       " 'comment:\\n\\n&gt;',\n",
       " 'I',\n",
       " 'think',\n",
       " 'that',\n",
       " 'women',\n",
       " 'tend',\n",
       " 'to',\n",
       " 'gravitate',\n",
       " 'towards',\n",
       " 'fem',\n",
       " 'gay',\n",
       " 'men',\n",
       " '',\n",
       " 'guys',\n",
       " 'who',\n",
       " 'want',\n",
       " 'to',\n",
       " 'be',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'girls,',\n",
       " 'so',\n",
       " 'to',\n",
       " 'speak.',\n",
       " 'The',\n",
       " 'other',\n",
       " 'gays,',\n",
       " 'who',\n",
       " 'are',\n",
       " 'more',\n",
       " 'stealth',\n",
       " 'because',\n",
       " 'they',\n",
       " 'are',\n",
       " 'straight',\n",
       " 'acting,',\n",
       " 'are',\n",
       " 'no',\n",
       " 'more',\n",
       " 'interesting',\n",
       " 'to',\n",
       " 'most',\n",
       " 'women',\n",
       " 'than',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'us.',\n",
       " 'I',\n",
       " 'think',\n",
       " 'there',\n",
       " 'is',\n",
       " 'an',\n",
       " 'opportunity',\n",
       " 'to',\n",
       " 'build',\n",
       " 'bridges',\n",
       " 'with',\n",
       " 'the',\n",
       " 'latter',\n",
       " 'group.',\n",
       " 'The',\n",
       " 'former',\n",
       " 'group',\n",
       " '',\n",
       " 'fem',\n",
       " 'gays',\n",
       " '',\n",
       " 'is',\n",
       " 'probably',\n",
       " 'always',\n",
       " 'going',\n",
       " 'to',\n",
       " 'side',\n",
       " 'with',\n",
       " 'the',\n",
       " 'feminists,',\n",
       " 'because',\n",
       " 'they',\n",
       " 'are',\n",
       " 'themselves',\n",
       " 'openly',\n",
       " 'feminine.\\n\\nThen',\n",
       " 'there',\n",
       " 'is',\n",
       " 'the',\n",
       " '3rd',\n",
       " 'response',\n",
       " '(from',\n",
       " 'fondueguy),',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'too',\n",
       " 'long',\n",
       " 'to',\n",
       " 'repost',\n",
       " 'here,',\n",
       " 'but',\n",
       " 'basically',\n",
       " 'says',\n",
       " 'that',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'reasons',\n",
       " 'heterosexual',\n",
       " 'men',\n",
       " 'fear',\n",
       " 'homosexuals,',\n",
       " 'and',\n",
       " 'any',\n",
       " 'hint',\n",
       " 'of',\n",
       " 'being',\n",
       " 'labeled',\n",
       " 'as',\n",
       " 'such,',\n",
       " 'is',\n",
       " 'that',\n",
       " 'women',\n",
       " 'ruthlessly',\n",
       " 'punish',\n",
       " 'men',\n",
       " 'for',\n",
       " 'prior',\n",
       " 'homosexual',\n",
       " 'activity.',\n",
       " '',\n",
       " 'The',\n",
       " 'reality',\n",
       " 'is',\n",
       " 'that',\n",
       " 'male',\n",
       " 'homosexuality,',\n",
       " 'especially',\n",
       " 'by',\n",
       " 'those',\n",
       " 'of',\n",
       " 'us',\n",
       " 'who',\n",
       " 'do',\n",
       " 'not',\n",
       " 'readily',\n",
       " 'conform',\n",
       " 'to',\n",
       " 'the',\n",
       " 'effeminate',\n",
       " 'stereotype,',\n",
       " 'is',\n",
       " 'a',\n",
       " 'critical',\n",
       " 'piece',\n",
       " 'in',\n",
       " 'the',\n",
       " 'puzzle',\n",
       " 'of',\n",
       " 'gender/sex',\n",
       " 'relationships',\n",
       " 'that',\n",
       " 'drive',\n",
       " 'many',\n",
       " 'MRA',\n",
       " 'issues.',\n",
       " '\\n\\nAlso,',\n",
       " 'the',\n",
       " 'comment',\n",
       " 'itself',\n",
       " 'is',\n",
       " 'in',\n",
       " 'response',\n",
       " 'to',\n",
       " 'an',\n",
       " 'article',\n",
       " 'with',\n",
       " 'a',\n",
       " 'very',\n",
       " 'detailed',\n",
       " 'description',\n",
       " 'of',\n",
       " 'why',\n",
       " 'gay',\n",
       " 'men',\n",
       " '(again,',\n",
       " 'ones',\n",
       " 'who',\n",
       " 'do',\n",
       " 'not',\n",
       " 'conform',\n",
       " 'to',\n",
       " 'effeminate',\n",
       " 'stereotypes',\n",
       " 'and',\n",
       " 'are',\n",
       " 'therefore',\n",
       " 'nonthreatening',\n",
       " 'to',\n",
       " 'women)',\n",
       " 'have',\n",
       " 'something',\n",
       " 'to',\n",
       " 'contribute',\n",
       " 'to',\n",
       " 'MRA.',\n",
       " 'Why',\n",
       " 'not?',\n",
       " \"There's\",\n",
       " 'not',\n",
       " 'really',\n",
       " 'any',\n",
       " 'reason',\n",
       " 'NOT',\n",
       " 'to',\n",
       " 'be',\n",
       " 'better',\n",
       " 'informed',\n",
       " 'in',\n",
       " 'regards',\n",
       " 'to',\n",
       " 'something',\n",
       " \"you'd\",\n",
       " 'like',\n",
       " 'to',\n",
       " 'support.',\n",
       " 'That',\n",
       " 'and',\n",
       " 'I',\n",
       " 'was',\n",
       " 'wondering',\n",
       " 'what',\n",
       " 'the',\n",
       " 'derogatory',\n",
       " 'term',\n",
       " 'for',\n",
       " 'circumcision',\n",
       " 'is.',\n",
       " '',\n",
       " 'Every',\n",
       " 'hotel',\n",
       " 'is',\n",
       " '\"a',\n",
       " 'luxury',\n",
       " 'hotel\"',\n",
       " 'if',\n",
       " 'you',\n",
       " 'read',\n",
       " 'their',\n",
       " 'marketing.\\n\\nHere',\n",
       " 'is',\n",
       " 'the',\n",
       " 'very',\n",
       " 'first',\n",
       " 'consumer',\n",
       " 'rating',\n",
       " 'from',\n",
       " 'trip',\n",
       " 'advisor',\n",
       " '(http://www.tripadvisor.co.uk/ShowUserReviews-g60763-d208454-r107556157-Sofitel_New_York-New_York_City_New_York.html):\\n\\n&gt;',\n",
       " 'Ideal',\n",
       " 'location.',\n",
       " 'Had',\n",
       " 'quite',\n",
       " 'a',\n",
       " 'large',\n",
       " 'room',\n",
       " 'for',\n",
       " 'the',\n",
       " 'NY',\n",
       " 'standard',\n",
       " '(same',\n",
       " 'for',\n",
       " 'the',\n",
       " 'bathroom).',\n",
       " 'The',\n",
       " 'staff',\n",
       " 'was',\n",
       " 'ok',\n",
       " 'but',\n",
       " 'nothing',\n",
       " 'more.',\n",
       " '**The',\n",
       " 'furnitures,',\n",
       " 'carpet',\n",
       " 'and',\n",
       " 'curtains',\n",
       " 'were',\n",
       " 'a',\n",
       " 'bit',\n",
       " 'tired',\n",
       " 'and',\n",
       " 'colors',\n",
       " 'were',\n",
       " 'sad.**',\n",
       " 'The',\n",
       " 'bar',\n",
       " 'is',\n",
       " 'not',\n",
       " 'the',\n",
       " \"Sofitel's\",\n",
       " 'bar',\n",
       " 'and',\n",
       " 'ordinary.',\n",
       " 'I',\n",
       " \"didn't\",\n",
       " 'experience',\n",
       " 'the',\n",
       " 'breakfast',\n",
       " 'as',\n",
       " 'it',\n",
       " 'was',\n",
       " 'too',\n",
       " 'expensive.',\n",
       " 'Compared',\n",
       " 'to',\n",
       " 'the',\n",
       " 'nearby',\n",
       " 'Novotel',\n",
       " '(which',\n",
       " 'has',\n",
       " 'a',\n",
       " 'very',\n",
       " 'nice',\n",
       " 'view',\n",
       " 'on',\n",
       " 'TImes',\n",
       " 'Square',\n",
       " 'for',\n",
       " 'some',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rooms),',\n",
       " \"**I'm\",\n",
       " 'not',\n",
       " 'conviced',\n",
       " 'it',\n",
       " 'disurves',\n",
       " 'its',\n",
       " '4',\n",
       " 'stars**',\n",
       " '...',\n",
       " 'There',\n",
       " 'are',\n",
       " 'so',\n",
       " 'many',\n",
       " 'other',\n",
       " 'nice',\n",
       " 'hotels',\n",
       " 'in',\n",
       " 'NYC',\n",
       " 'that',\n",
       " 'I',\n",
       " 'would',\n",
       " 'not',\n",
       " 'recommend',\n",
       " 'this',\n",
       " 'one\\n\\nI',\n",
       " 'have',\n",
       " 'seen',\n",
       " 'the',\n",
       " 'pictures',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rooms,',\n",
       " 'and',\n",
       " \"it's\",\n",
       " 'like',\n",
       " 'a',\n",
       " '3-star',\n",
       " 'hotel',\n",
       " '-',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'like',\n",
       " 'a',\n",
       " 'Holiday',\n",
       " 'Inn..\\n\\nFrankly,',\n",
       " 'I',\n",
       " 'stay',\n",
       " 'in',\n",
       " 'better',\n",
       " 'hotels',\n",
       " 'when',\n",
       " 'I',\n",
       " 'travel..',\n",
       " 'You',\n",
       " 'know,',\n",
       " 'you',\n",
       " 'are',\n",
       " 'probably',\n",
       " 'very',\n",
       " 'close',\n",
       " 'to',\n",
       " 'the',\n",
       " 'truth.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'assuming',\n",
       " 'direct',\n",
       " 'monetary',\n",
       " 'gain',\n",
       " 'from',\n",
       " 'court',\n",
       " 'settlements,',\n",
       " 'but',\n",
       " 'think',\n",
       " 'about',\n",
       " 'what',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'political',\n",
       " 'weapon',\n",
       " 'false',\n",
       " 'rape',\n",
       " 'accusations',\n",
       " 'can',\n",
       " 'be',\n",
       " 'used',\n",
       " 'as.',\n",
       " 'Politician',\n",
       " 'causing',\n",
       " 'you',\n",
       " 'issues?',\n",
       " 'Pay',\n",
       " 'a',\n",
       " 'female',\n",
       " 'in',\n",
       " 'his',\n",
       " 'entourage',\n",
       " 'to',\n",
       " 'accuse',\n",
       " 'him',\n",
       " 'of',\n",
       " 'rape.',\n",
       " '\\n\\nDisgusting.',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " 'i',\n",
       " 'truely',\n",
       " 'hate',\n",
       " 'the',\n",
       " 'IMF,',\n",
       " 'i',\n",
       " 'was',\n",
       " 'very',\n",
       " 'suspicious',\n",
       " 'of',\n",
       " 'this',\n",
       " 'to',\n",
       " 'begin',\n",
       " 'with.',\n",
       " 'these',\n",
       " 'types',\n",
       " 'of',\n",
       " 'cases',\n",
       " 'are',\n",
       " 'incredibly',\n",
       " 'effective',\n",
       " 'political',\n",
       " 'weapons.',\n",
       " 'if',\n",
       " 'he',\n",
       " 'did',\n",
       " 'it,',\n",
       " 'lock',\n",
       " 'him',\n",
       " 'up',\n",
       " 'and',\n",
       " 'throw',\n",
       " 'away',\n",
       " 'the',\n",
       " 'key.',\n",
       " 'if',\n",
       " 'he',\n",
       " \"didn't,\",\n",
       " 'well',\n",
       " 'im',\n",
       " 'not',\n",
       " 'gonna',\n",
       " 'cry',\n",
       " 'over',\n",
       " 'the',\n",
       " 'head',\n",
       " 'of',\n",
       " 'the',\n",
       " 'IMF',\n",
       " 'Because',\n",
       " 'most',\n",
       " 'people',\n",
       " 'have',\n",
       " 'pride',\n",
       " 'in',\n",
       " 'themselves,',\n",
       " 'and',\n",
       " 'would',\n",
       " 'rather',\n",
       " 'succeed',\n",
       " 'on',\n",
       " 'their',\n",
       " 'own',\n",
       " 'accord',\n",
       " 'than',\n",
       " 'have',\n",
       " 'government',\n",
       " 'give',\n",
       " 'them',\n",
       " 'a',\n",
       " 'handout.\\n\\nWell,',\n",
       " '*most*',\n",
       " 'people,',\n",
       " 'that',\n",
       " 'is.',\n",
       " 'Many',\n",
       " 'feminists',\n",
       " 'do',\n",
       " 'hate',\n",
       " 'men',\n",
       " 'and',\n",
       " 'want',\n",
       " 'to',\n",
       " 'emasculate',\n",
       " 'them.',\n",
       " 'While',\n",
       " \"I'm\",\n",
       " 'thankful',\n",
       " 'for',\n",
       " 'the',\n",
       " 'few',\n",
       " 'who',\n",
       " \"don't\",\n",
       " 'I',\n",
       " 'feel',\n",
       " 'that',\n",
       " 'their',\n",
       " 'silence',\n",
       " 'allows',\n",
       " 'the',\n",
       " 'groups',\n",
       " 'like',\n",
       " 'NOW',\n",
       " 'to',\n",
       " 'exploit',\n",
       " 'men',\n",
       " 'and',\n",
       " 'women',\n",
       " 'alike',\n",
       " 'for',\n",
       " 'their',\n",
       " 'own',\n",
       " 'aims.',\n",
       " 'No',\n",
       " 'mainstream',\n",
       " 'feminist',\n",
       " 'organisation',\n",
       " 'has',\n",
       " 'ever',\n",
       " 'fought',\n",
       " 'for',\n",
       " 'a',\n",
       " 'substantive',\n",
       " 'issues',\n",
       " 'for',\n",
       " 'men.',\n",
       " 'This',\n",
       " 'has',\n",
       " 'allowed',\n",
       " 'large',\n",
       " 'inequalities',\n",
       " 'to',\n",
       " 'arise.',\n",
       " 'Instead',\n",
       " 'of',\n",
       " 'challenging',\n",
       " 'them,',\n",
       " 'those',\n",
       " 'organisations',\n",
       " 'fight',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'them',\n",
       " 'as',\n",
       " 'they',\n",
       " 'benefit',\n",
       " 'women.\\n\\nSo',\n",
       " 'perhaps',\n",
       " 'you',\n",
       " \"aren't\",\n",
       " 'a',\n",
       " 'misandrist.',\n",
       " 'But',\n",
       " 'all',\n",
       " 'the',\n",
       " 'people',\n",
       " '\"in',\n",
       " 'charge\"',\n",
       " 'of',\n",
       " 'feminism',\n",
       " 'are.',\n",
       " 'It',\n",
       " 'might',\n",
       " 'be',\n",
       " 'time',\n",
       " 'to',\n",
       " 'adopt',\n",
       " 'a',\n",
       " 'new',\n",
       " 'title',\n",
       " 'for',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictionary which has indices and unique word. \n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(words)\n",
    "# Make vectors. \n",
    "vectors = [dictionary.doc2bow(text) for text in words]\n",
    "# TF-IDF\n",
    "from gensim.models import TfidfModel\n",
    "tfidf = TfidfModel(vectors)\n",
    "# Use the first doc for word cloud. \n",
    "weights = tfidf[vectors[0]]\n",
    "# Get terms from the dictionary and pair with weights. \n",
    "weights = [(dictionary[pair[0]], pair[1]) for pair in weights]\n",
    "# Convert weights to dictionary. \n",
    "weights_dict = {}\n",
    "for k, v in weights:\n",
    "    weights_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the word cloud. \n",
    "\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=2000,\n",
    "    width = 1024,\n",
    "    height = 720,\n",
    ")\n",
    "\n",
    "# Generate the cloud. \n",
    "word_cloud.generate_from_frequencies(weights_dict)\n",
    "# Save the word cloud as a file. \n",
    "word_cloud.to_file(\"word_cloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|     id|                body|\n",
      "+-------+--------------------+\n",
      "|dbumo2q|The one analogy I...|\n",
      "|dbumoz0|&gt; peace, blue ...|\n",
      "|dbumpim|All of a sudden, ...|\n",
      "|dbumq8n|HTML5 was beginni...|\n",
      "|dbumqf6|Turn off notifica...|\n",
      "|dbumtjp|&gt; The mbp has ...|\n",
      "|dbumtju|I think one thing...|\n",
      "|dbumv4w|I'm including ord...|\n",
      "|dbumxhj|Definitely looks ...|\n",
      "|dbumy8v|Ah yes spec sheet...|\n",
      "|dbumzys|admin work should...|\n",
      "|dbun16b|Idk if you're bei...|\n",
      "|dbun1cu|See my top post f...|\n",
      "|dbun1o7|Hello! Unfortunat...|\n",
      "|dbun1z1|Here in Belgium t...|\n",
      "|dbun2ci|Don't let the hat...|\n",
      "|dbun4g3|Quick question......|\n",
      "|dbun5ek|&gt; Lets see som...|\n",
      "|dbun5jx|I use mine to pay...|\n",
      "|dbun6c7|Yeah, because fir...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "SELECT\n",
    "  id, \n",
    "  body\n",
    "FROM\n",
    "  reddit_data\n",
    "WHERE\n",
    "  subreddit = \"technology\"\n",
    "  AND\n",
    "  body != \"[deleted]\"\n",
    "'''\n",
    "text_tech = spark.sql(query)\n",
    "text_tech.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                body|\n",
      "+--------------------+\n",
      "|The one analogy I...|\n",
      "|&gt; peace, blue ...|\n",
      "|All of a sudden, ...|\n",
      "|HTML5 was beginni...|\n",
      "|Turn off notifica...|\n",
      "|&gt; The mbp has ...|\n",
      "|I think one thing...|\n",
      "|I'm including ord...|\n",
      "|Definitely looks ...|\n",
      "|Ah yes spec sheet...|\n",
      "|admin work should...|\n",
      "|Idk if you're bei...|\n",
      "|See my top post f...|\n",
      "|Hello! Unfortunat...|\n",
      "|Here in Belgium t...|\n",
      "|Don't let the hat...|\n",
      "|Quick question......|\n",
      "|&gt; Lets see som...|\n",
      "|I use mine to pay...|\n",
      "|Yeah, because fir...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_tech.select(\"body\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The one analogy I though of was that, back then, we were like satelites, or rockets: Only a handful of manufactures that could dictate the rules.\\n\\nSo it felt we were in control, forever and ever, but thats all we were, the only hot girl in the sausage party.\\n\\nNow a days were like the clothing store, if they can, they will shop online, but otherwise, in a hurry, or to someone that has some extra money will buy locally.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tech.head().body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, body: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_tech.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (text_tech\n",
    "  .rdd\n",
    "  .map(lambda x : (x.id, x.body.split(\" \")))\n",
    "  .toDF()\n",
    "  .withColumnRenamed(\"_1\",\"doc_id\")\n",
    "  .withColumnRenamed(\"_2\",\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "| doc_id|            features|\n",
      "+-------+--------------------+\n",
      "|dbumo2q|[The, one, analog...|\n",
      "|dbumoz0|[&gt;, peace,, bl...|\n",
      "|dbumpim|[All, of, a, sudd...|\n",
      "|dbumq8n|[HTML5, was, begi...|\n",
      "|dbumqf6|[Turn, off, notif...|\n",
      "|dbumtjp|[&gt;, The, mbp, ...|\n",
      "|dbumtju|[I, think, one, t...|\n",
      "|dbumv4w|[I'm, including, ...|\n",
      "|dbumxhj|[Definitely, look...|\n",
      "|dbumy8v|[Ah, yes, spec, s...|\n",
      "|dbumzys|[admin, work, sho...|\n",
      "|dbun16b|[Idk, if, you're,...|\n",
      "|dbun1cu|[See, my, top, po...|\n",
      "|dbun1o7|[Hello!, Unfortun...|\n",
      "|dbun1z1|[Here, in, Belgiu...|\n",
      "|dbun2ci|[Don't, let, the,...|\n",
      "|dbun4g3|[Quick, question....|\n",
      "|dbun5ek|[&gt;, Lets, see,...|\n",
      "|dbun5jx|[I, use, mine, to...|\n",
      "|dbun6c7|[Yeah,, because, ...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [row[\"features\"] for row in df.rdd.collect()]\n",
    "# words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dictionary which has indices and unique word. \n",
    "from gensim.corpora import Dictionary\n",
    "dictionary = Dictionary(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make vectors. \n",
    "vectors = [dictionary.doc2bow(text) for text in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TF-IDF\n",
    "from gensim.models import TfidfModel\n",
    "tfidf = TfidfModel(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first doc for word cloud. \n",
    "weights = tfidf[vectors[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get terms from the dictionary and pair with weights. \n",
    "weights = [(dictionary[pair[0]], pair[1]) for pair in weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert weights to dictionary. \n",
    "weights_dict = {}\n",
    "for k, v in weights:\n",
    "    weights_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f83157f7668>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the word cloud. \n",
    "\n",
    "from wordcloud import WordCloud\n",
    "word_cloud = WordCloud(\n",
    "    background_color=\"white\",\n",
    "    max_words=2000,\n",
    "    width = 1024,\n",
    "    height = 720,\n",
    ")\n",
    "\n",
    "# Generate the cloud. \n",
    "word_cloud.generate_from_frequencies(weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x7f83157f7668>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the word cloud as a file. \n",
    "word_cloud.to_file(\"word_cloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "# # TF\n",
    "# htf = HashingTF(inputCol=\"features\", outputCol=\"tf\")\n",
    "# tf = htf.transform(df)\n",
    "# tf.show(truncate=False)\n",
    "# # tf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TF-IDF\n",
    "# idf = IDF(inputCol=\"tf\", outputCol=\"idf\")\n",
    "# tfidf = idf.fit(tf).transform(tf)\n",
    "# tfidf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
